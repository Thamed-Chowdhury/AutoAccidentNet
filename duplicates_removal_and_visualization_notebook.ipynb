{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV file containing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the API key first. Only 1 API key is allowed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"Enter your API key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "duplicates_df = pd.read_csv(\"Duplicate_removal/Duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the responses from combined response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['Report Type', 'Publish Date','Accident Date','Time of Accident','Killed','Injured','Location','Road Type', 'Pedestrian Involved', 'Vehicles Involced', 'District']  # Generate column names\n",
    "# Split the \"Gemini_responses\" column and handle discrepancies\n",
    "split_df = duplicates_df[\"LLM Response\"].str.split(\"<sep>\", expand=True)\n",
    "\n",
    "# Ensure the resulting DataFrame has the same number of columns as `new_columns`\n",
    "split_df = split_df.reindex(columns=range(len(new_columns)), fill_value=\"ERROR\")\n",
    "\n",
    "# Assign the split data to the new columns\n",
    "duplicates_df[new_columns] = split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unwanted spaces from 'District' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df['District'] = duplicates_df['District'].str.replace('\\n', ' ', regex=True).str.strip()\n",
    "# duplicates_df = duplicates_df[duplicates_df.duplicated(subset=['Publish Date', 'Accident Date', 'District'], keep=False)]\n",
    "# duplicates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplication Algorithm #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Do NOT filter out unique news. Use the complete dataframe.\n",
    "# duplicates_df = duplicates_df[duplicates_df.duplicated(subset=['Publish Date', 'District'], keep=False)]\n",
    "# duplicates_df.head()\n",
    "\n",
    "# Initialize the Gemini API with your API key\n",
    "genai.configure(api_key=API_KEY)  # Replace with your actual API key\n",
    "\n",
    "def call_gemini_api(base_title, base_description, candidate_texts):\n",
    "    \"\"\"\n",
    "    Uses the Gemini API to check if candidate articles report the same accident.\n",
    "    The prompt now includes both the news title and description.\n",
    "    \"\"\"\n",
    "    base_news_text = f\"News Title: {base_title}\\nNews Description: {base_description}\"\n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt = f\"Base news:\\n{base_news_text}\\n\\n\"\n",
    "    prompt += (\"For each of the following news articles, determine if it reports the same accident \"\n",
    "               \"as the base news. Answer 'True' if yes, and 'False' if not.\\n\\n\")\n",
    "    for idx, candidate in enumerate(candidate_texts, start=1):\n",
    "        prompt += f\"{idx}. {candidate}\\n\\n\"\n",
    "    \n",
    "    # Create model instance\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "    \n",
    "    # Generate content\n",
    "    response = model.generate_content(prompt)\n",
    "    print(response.text)\n",
    "    \n",
    "    # Get the text response from the API\n",
    "    answer_text = response.text\n",
    "    \n",
    "    # Parse the response to extract boolean answers\n",
    "    answers = []\n",
    "    for line in answer_text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \"True\" in line:\n",
    "            answers.append(True)\n",
    "        elif \"False\" in line:\n",
    "            answers.append(False)\n",
    "    \n",
    "    # Handle mismatched answer counts by assuming non-duplicates if the count is off\n",
    "    if len(answers) != len(candidate_texts):\n",
    "        print(f\"Warning: Expected {len(candidate_texts)} answers, got {len(answers)}. Assuming non-duplicates.\")\n",
    "        answers = [False] * len(candidate_texts)\n",
    "        \n",
    "    return answers\n",
    "\n",
    "def process_group(group_df):\n",
    "    \"\"\"\n",
    "    Process one group of potential duplicates using an iterative LLM-powered approach.\n",
    "    \"\"\"\n",
    "    df_unique_group = pd.DataFrame(columns=group_df.columns)\n",
    "    candidates = group_df.copy().reset_index(drop=True)\n",
    "    \n",
    "    while not candidates.empty:\n",
    "        if df_unique_group.empty:\n",
    "            # Use the first candidate as the base news\n",
    "            base_row = candidates.iloc[0]\n",
    "            df_unique_group = pd.concat(\n",
    "                [df_unique_group, base_row.to_frame().T], \n",
    "                ignore_index=True\n",
    "            )\n",
    "            candidates = candidates.drop(candidates.index[0]).reset_index(drop=True)\n",
    "        else:\n",
    "            base_row = df_unique_group.iloc[-1]\n",
    "        \n",
    "        if candidates.empty:\n",
    "            break\n",
    "        \n",
    "        base_title = base_row['News Title']\n",
    "        base_description = base_row['Description']\n",
    "        \n",
    "        # Prepare candidate texts by combining title and description for each candidate\n",
    "        candidate_texts = []\n",
    "        for _, row in candidates.iterrows():\n",
    "            candidate_text = f\"News Title: {row['News Title']}\\nNews Description: {row['Description']}\"\n",
    "            candidate_texts.append(candidate_text)\n",
    "        \n",
    "        duplicate_flags = call_gemini_api(base_title, base_description, candidate_texts)\n",
    "        \n",
    "        # Filter out duplicates based on the LLM's response\n",
    "        non_duplicate_indices = [idx for idx, is_dup in enumerate(duplicate_flags) if not is_dup]\n",
    "        candidates = candidates.iloc[non_duplicate_indices].reset_index(drop=True)\n",
    "        \n",
    "        if not candidates.empty:\n",
    "            next_row = candidates.iloc[0]\n",
    "            df_unique_group = pd.concat(\n",
    "                [df_unique_group, next_row.to_frame().T], \n",
    "                ignore_index=True\n",
    "            )\n",
    "            candidates = candidates.drop(candidates.index[0]).reset_index(drop=True)\n",
    "    \n",
    "    return df_unique_group\n",
    "\n",
    "def process_accidents(df):\n",
    "    unique_groups = []\n",
    "    for (district, accident_date), group in df.groupby(['District', 'Accident Date']):\n",
    "        unique_group = process_group(group)\n",
    "        unique_groups.append(unique_group)\n",
    "\n",
    "    # If no groups were processed, return an empty dataframe with the same columns\n",
    "    if not unique_groups:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    return pd.concat(unique_groups, ignore_index=True)\n",
    "\n",
    "def process_accidents_publish_date(df):\n",
    "    unique_groups = []\n",
    "    for (district, publish_date), group in df.groupby(['District', 'Publish Date']):\n",
    "        unique_group = process_group(group)\n",
    "        unique_groups.append(unique_group)\n",
    "\n",
    "    if not unique_groups:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    return pd.concat(unique_groups, ignore_index=True)\n",
    "\n",
    "# Assume duplicates_df contains both duplicate and unique news records\n",
    "if len(duplicates_df) != 0:\n",
    "    # Load your dataframe (replace with actual data loading)\n",
    "    # df = pd.read_csv('your_data.csv')\n",
    "    df = duplicates_df  # Replace with your actual dataframe\n",
    "\n",
    "    # Process duplicates\n",
    "    df_unique_temp = process_accidents(df)\n",
    "    df_unique = process_accidents_publish_date(df_unique_temp)\n",
    "\n",
    "    print(\"Unique accident news:\")\n",
    "    print(df_unique.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_unique contains only unique entries without any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing and Heatmap Generation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both columns to integers\n",
    "df_unique['Killed'] = pd.to_numeric(df_unique['Killed'], errors='coerce').fillna(0).astype(int)\n",
    "df_unique['Injured'] = pd.to_numeric(df_unique['Injured'], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Read the GeoJSON file into a GeoDataFrame\n",
    "gdf = gpd.read_file('Duplicate_removal/Bangladesh_GeoJSON/bangladesh_geojson_adm2_64_districts_zillas.json')\n",
    "\n",
    "# Display the first few rows of the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# (Optional) Quick plot to visually inspect the boundaries\n",
    "gdf.plot(figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Prepare Your DataFrame\n",
    "# -------------------------------\n",
    "# data = {\n",
    "#     'District': [\n",
    "#         'Bagerhat', 'Bandarban', 'Barguna', 'Barisal', 'Bhola', \n",
    "#         'Bagerhat', 'Barguna', 'Bandarban', 'Barisal', 'Bagerhat'\n",
    "#     ]\n",
    "# }\n",
    "# df_unique = pd.DataFrame(data)\n",
    "\n",
    "# Count occurrences for each district\n",
    "df_counts = df_unique['District'].value_counts().reset_index()\n",
    "df_counts.columns = ['District', 'count']\n",
    "\n",
    "print(\"District counts:\\n\", df_counts)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Load the GeoJSON File\n",
    "# -------------------------------\n",
    "gdf = gpd.read_file('Duplicate_removal/Bangladesh_GeoJSON/bangladesh_geojson_adm2_64_districts_zillas.json')\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Merge Data with GeoDataFrame\n",
    "# -------------------------------\n",
    "merged = gdf.merge(df_counts, left_on='ADM2_EN', right_on='District', how='left')\n",
    "merged['count'] = merged['count'].fillna(0)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Plot the Choropleth Map\n",
    "# -------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "merged.plot(\n",
    "    column='count',\n",
    "    cmap='OrRd',\n",
    "    linewidth=0.8,\n",
    "    ax=ax,\n",
    "    edgecolor='0.8',\n",
    "    legend=True\n",
    ")\n",
    "\n",
    "ax.set_title('Heatmap of Districtwise Accident Occurrence in Bangladesh', fontsize=15)\n",
    "ax.set_axis_off()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Annotate District Names\n",
    "# -------------------------------\n",
    "for idx, row in merged.iterrows():\n",
    "    # Use a representative point to ensure the label appears inside the polygon\n",
    "    rep_point = row['geometry'].representative_point()\n",
    "    # Pass the text as the first positional argument instead of using s=\n",
    "    ax.annotate(row['ADM2_EN'], (rep_point.x, rep_point.y),\n",
    "                horizontalalignment='center', fontsize=6, color='black')\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Display the Map\n",
    "# -------------------------------\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load Your Data\n",
    "# -------------------------------\n",
    "# Sample DataFrame with district-wise fatalities and injuries\n",
    "# data = {\n",
    "#     'District': ['Bagerhat', 'Bandarban', 'Barguna', 'Barisal', 'Bhola', \n",
    "#                  'Bagerhat', 'Barguna', 'Bandarban', 'Barisal', 'Bagerhat'],\n",
    "#     'Killed':   [5, 3, 8, 2, 6, 7, 4, 1, 3, 9],\n",
    "#     'Injured':  [10, 5, 15, 8, 12, 14, 6, 3, 7, 11]\n",
    "# }\n",
    "# df_unique = pd.DataFrame(data)\n",
    "\n",
    "# Aggregate the total 'Killed' and 'Injured' counts for each district\n",
    "df_counts = df_unique.groupby('District').sum().reset_index()\n",
    "\n",
    "print(\"Aggregated district data:\\n\", df_counts)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Load the GeoJSON File\n",
    "# -------------------------------\n",
    "gdf = gpd.read_file('Duplicate_removal/Bangladesh_GeoJSON/bangladesh_geojson_adm2_64_districts_zillas.json')  # Update path\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Merge Data with GeoDataFrame\n",
    "# -------------------------------\n",
    "merged = gdf.merge(df_counts, left_on='ADM2_EN', right_on='District', how='left')\n",
    "merged[['Killed', 'Injured']] = merged[['Killed', 'Injured']].fillna(0)  # Fill missing values with 0\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Define a Function to Plot Heatmaps\n",
    "# -------------------------------\n",
    "def plot_heatmap(geo_df, column, title, cmap):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    geo_df.plot(\n",
    "        column=column,\n",
    "        cmap=cmap,\n",
    "        linewidth=0.8,\n",
    "        ax=ax,\n",
    "        edgecolor='0.8',\n",
    "        legend=True\n",
    "    )\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Add district names on the map\n",
    "    for idx, row in geo_df.iterrows():\n",
    "        rep_point = row['geometry'].representative_point()\n",
    "        ax.annotate(row['ADM2_EN'], (rep_point.x, rep_point.y), \n",
    "                    horizontalalignment='center', fontsize=8, color='black')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Generate Two Heatmaps\n",
    "# -------------------------------\n",
    "plot_heatmap(merged, 'Killed', 'Heatmap of Fatalities in Bangladesh', cmap='Reds')\n",
    "plot_heatmap(merged, 'Injured', 'Heatmap of Injuries in Bangladesh', cmap='Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Initialize Google GenAI (ensure your API key is set up)\n",
    "genai.configure(api_key=\"AIzaSyBr9HVc7wmhRk3yqKygq_pwKjgCwj2U19k\")\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "def normalize_time(time_str):\n",
    "    if pd.isna(time_str) or time_str.lower() in [\"not mentioned\", \"unknown\", \"n/a\", \"-\"]:\n",
    "        return None\n",
    "    prompt = f\"Convert the following time description into a 24-hour format (HH:MM). Provide only the time in HH:MM format and nothing else: {time_str}\"\n",
    "    response = model.generate_content(prompt)\n",
    "    extracted_time = re.search(r'\\b\\d{1,2}:\\d{2}\\b', response.text)\n",
    "    return extracted_time.group(0) if extracted_time else None\n",
    "\n",
    "# # Sample DataFrame with mixed time formats\n",
    "# data = {'Time of Accident': [\"7:00 AM\", \"3:00 PM\", \"Afternoon\", \"morning\", \"11:45 PM\", \"Noon\", \"Midnight\", \"not mentioned\", None]}\n",
    "# df_unique = pd.DataFrame(data)\n",
    "\n",
    "# Normalize time using GenAI\n",
    "df_unique['Normalized Time'] = df_unique['Time of Accident'].apply(normalize_time)\n",
    "\n",
    "# Remove rows with missing or unrecognized time\n",
    "df_unique = df_unique.dropna(subset=['Normalized Time'])\n",
    "\n",
    "# Convert to datetime format\n",
    "df_unique['Hour'] = pd.to_datetime(df_unique['Normalized Time'], format='%H:%M', errors='coerce').dt.hour\n",
    "\n",
    "# Remove NaN values caused by conversion errors\n",
    "df_unique = df_unique.dropna(subset=['Hour'])\n",
    "\n",
    "# Convert hour to integer\n",
    "df_unique['Hour'] = df_unique['Hour'].astype(int)\n",
    "\n",
    "# Create hourly bins\n",
    "time_bins = [f\"{i:02d}:00-{i+1:02d}:00\" for i in range(24)]\n",
    "hourly_counts = df_unique['Hour'].value_counts().reindex(range(24), fill_value=0)\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(time_bins, hourly_counts.values, color='royalblue', alpha=0.7)\n",
    "plt.xlabel(\"Time of Day (Hourly Intervals)\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.title(\"Temporal Distribution of Accidents\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
